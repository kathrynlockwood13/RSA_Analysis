{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/glm_pri/lib/python3.7/site-packages/nilearn/__init__.py:69: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    },
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(15000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "import nilearn as nil \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import scipy.stats \n",
    "import nibabel as nib \n",
    "import os \n",
    "%autosave 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title Blurb: run1 \n",
    "#Voxel Data Extraction and Analysis Script\"\n",
    "\n",
    "#Code Explanation:\n",
    "#This script is designed for the extraction and analysis of voxel data from neuroimaging data stored in NIfTI (Neuroimaging Informatics Technology Initiative) format. \n",
    "#It processes data for multiple subjects, regions of interest (ROIs), and runs. Key functionalities include loading NIfTI images, extracting voxel data within specified ROIs, organizing the data into DataFrames, \n",
    "#and saving the results as CSV files for subsequent analysis. \n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/data/Wangjing/projects/priority/derivatives/LSS/ses-02_task-learning/outputs_moreclean'\n",
    "roi_path_base = '/data/Wangjing/projects/priority/derivatives/ROIs'\n",
    "runs = [\"01\"]\n",
    "trials = [\"%02d\" % x for x in range(1, 55)]\n",
    "regions = ['vtc', 'peri', 'rsp']\n",
    "sub_list = [\"%02d\" % x for x in range(2, 31)]  # For all subjects from 02 to 30\n",
    "\n",
    "region_path_map = {\n",
    "    'vtc': 'vtc',\n",
    "    'peri': 'peri',\n",
    "    'rsp': 'rsp_all',\n",
    "}\n",
    "\n",
    "for sub_id in sub_list:\n",
    "    for region in regions:\n",
    "        for run in runs:\n",
    "            all_voxels = []\n",
    "\n",
    "            for trial in trials:\n",
    "                voxel_values = []\n",
    "\n",
    "                image_path = os.path.join(data_path, f\"sub-{sub_id}/run-{run}/t-{trial}/cope1.nii.gz\")\n",
    "                \n",
    "                if region == 'rsp':\n",
    "                    roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/Schaefer/2mm/{region_path_map[region]}.nii.gz\")\n",
    "                else:\n",
    "                    roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/freesurfer/anat/2mm/{region_path_map[region]}_bin.nii.gz\")\n",
    "                \n",
    "                image = nib.load(image_path)\n",
    "                roi_image = nib.load(roi_path)\n",
    "\n",
    "                data = image.get_fdata()\n",
    "                roi_data = roi_image.get_fdata()\n",
    "\n",
    "                mask = np.where(roi_data == 1)\n",
    "                roi_voxels = data[mask].flatten()  # Flattening voxel data into 1D\n",
    "\n",
    "                voxel_values.extend(roi_voxels)\n",
    "\n",
    "                all_voxels.append(voxel_values)\n",
    "\n",
    "            df = pd.DataFrame(all_voxels).transpose()  # Transpose to get voxels as rows\n",
    "            df.columns = ['trial_%s' % t for t in trials]  # Rename columns\n",
    "            df.index = ['voxel_%d' % (i+1) for i in range(df.shape[0])]  # Adding index names\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            output_dir = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{sub_id}'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Save DataFrame to the specified path for each run\n",
    "            output_path = os.path.join(output_dir, f'voxBYtrial_{region}_sub-{sub_id}_run-{run}_day2enc.csv')\n",
    "            df.to_csv(output_path)\n",
    "\n",
    "        print(f\"CSVs generated for Subject {sub_id} for region {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title Blurb: run2\n",
    "#Voxel Data Extraction and Analysis Script\"\n",
    "\n",
    "#Code Explanation:\n",
    "#This script is designed for the extraction and analysis of voxel data from neuroimaging data stored in NIfTI (Neuroimaging Informatics Technology Initiative) format. \n",
    "#It processes data for multiple subjects, regions of interest (ROIs), and runs. Key functionalities include loading NIfTI images, extracting voxel data within specified ROIs, organizing the data into DataFrames, \n",
    "#and saving the results as CSV files for subsequent analysis. \n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/data/Wangjing/projects/priority/derivatives/LSS/ses-02_task-learning/outputs_moreclean'\n",
    "roi_path_base = '/data/Wangjing/projects/priority/derivatives/ROIs'\n",
    "runs = [\"02\"]\n",
    "trials = [\"%02d\" % x for x in range(1, 55)]\n",
    "regions = ['vtc', 'peri', 'rsp']\n",
    "sub_list = [\"%02d\" % x for x in range(2, 31)]  # For all subjects from 02 to 30\n",
    "\n",
    "region_path_map = {\n",
    "    'vtc': 'vtc',\n",
    "    'peri': 'peri',\n",
    "    'rsp': 'rsp_all',\n",
    "}\n",
    "\n",
    "for sub_id in sub_list:\n",
    "    for region in regions:\n",
    "        for run in runs:\n",
    "            all_voxels = []\n",
    "\n",
    "            for trial in trials:\n",
    "                voxel_values = []\n",
    "\n",
    "                image_path = os.path.join(data_path, f\"sub-{sub_id}/run-{run}/t-{trial}/cope1.nii.gz\")\n",
    "                \n",
    "                if region == 'rsp':\n",
    "                    roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/Schaefer/2mm/{region_path_map[region]}.nii.gz\")\n",
    "                else:\n",
    "                    roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/freesurfer/anat/2mm/{region_path_map[region]}_bin.nii.gz\")\n",
    "                \n",
    "                image = nib.load(image_path)\n",
    "                roi_image = nib.load(roi_path)\n",
    "\n",
    "                data = image.get_fdata()\n",
    "                roi_data = roi_image.get_fdata()\n",
    "\n",
    "                mask = np.where(roi_data == 1)\n",
    "                roi_voxels = data[mask].flatten()  # Flattening voxel data into 1D\n",
    "\n",
    "                voxel_values.extend(roi_voxels)\n",
    "\n",
    "                all_voxels.append(voxel_values)\n",
    "\n",
    "            df = pd.DataFrame(all_voxels).transpose()  # Transpose to get voxels as rows\n",
    "            df.columns = ['trial_%s' % t for t in trials]  # Rename columns\n",
    "            df.index = ['voxel_%d' % (i+1) for i in range(df.shape[0])]  # Adding index names\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            output_dir = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{sub_id}'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Save DataFrame to the specified path for each run\n",
    "            output_path = os.path.join(output_dir, f'voxBYtrial_{region}_sub-{sub_id}_run-{run}_day2enc.csv')\n",
    "            df.to_csv(output_path)\n",
    "\n",
    "        print(f\"CSVs generated for Subject {sub_id} for region {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest day 2 rest run 1,2, and 3 \n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/data/Wangjing/projects/priority/derivatives/classification'\n",
    "roi_path_base = '/data/Wangjing/projects/priority/derivatives/ROIs'\n",
    "\n",
    "#\n",
    "sub_list = [\"%02d\" % x for x in range(2, 31)]  # Subjects from 02 to 30\n",
    "\n",
    "# List of regions\n",
    "regions = ['vtc', 'peri', 'rsp']\n",
    "\n",
    "all_voxels = {}\n",
    "\n",
    "for sub_id in sub_list:\n",
    "    for region in regions:\n",
    "        for run_id in [\"01\", \"02\", \"03\"]:  # For the 3 runs\n",
    "            image_path = os.path.join(data_path, f\"sub-{sub_id}/ses-02/task-rest/cleaned_img/cleaned_task-rest_run-{run_id}.nii.gz\")  # 4D fMRI data\n",
    "\n",
    "            if region == 'rsp':\n",
    "                roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/Schaefer/2mm/{region}_all.nii.gz\")\n",
    "            else:\n",
    "                roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/freesurfer/anat/2mm/{region}_bin.nii.gz\")  # 3D ROI for the given region\n",
    "\n",
    "            image = nib.load(image_path)\n",
    "            roi_image = nib.load(roi_path)\n",
    "\n",
    "            data = image.get_fdata()\n",
    "            roi_data = roi_image.get_fdata()\n",
    "\n",
    "            mask = np.where(roi_data == 1)\n",
    "\n",
    "            voxel_list = []\n",
    "            for t in range(data.shape[-1]):  # For each TR\n",
    "                roi_voxels = data[..., t][mask]  # Extracting voxel values at the current TR\n",
    "                voxel_list.append(roi_voxels)\n",
    "\n",
    "            key = (sub_id, region, run_id)\n",
    "            all_voxels[key] = voxel_list\n",
    "\n",
    "# Output voxel data as separate CSVs\n",
    "for (sub_id, region, run_id), voxels in all_voxels.items():\n",
    "    df = pd.DataFrame(voxels).transpose()  # Transpose dataframe\n",
    "    df.columns = ['TR_%d' % (i+1) for i in range(df.shape[1])]  # Adding column names\n",
    "    df.index = ['voxel_%d' % (i+1) for i in range(df.shape[0])]  # Adding index names\n",
    "\n",
    "    output_dir = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{sub_id}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f'voxBYtr_{region}_sub-{sub_id}_run-{run_id}_day2rest.csv')\n",
    "    df.to_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest day 1 rest run 1 \n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/data/Wangjing/projects/priority/derivatives/classification'\n",
    "roi_path_base = '/data/Wangjing/projects/priority/derivatives/ROIs'\n",
    "\n",
    "# List of subjects (only subject 2)\n",
    "sub_list = [\"%02d\" % x for x in range(2, 31)]  # Subjects from 02 to 30\n",
    "\n",
    "# List of regions\n",
    "regions = ['vtc', 'peri', 'rsp']\n",
    "\n",
    "all_voxels = {}\n",
    "\n",
    "for sub_id in sub_list:\n",
    "    for region in regions:\n",
    "        for run_id in [\"01\"]:  # day1 baseline\n",
    "            image_path = os.path.join(data_path, f\"sub-{sub_id}/ses-01/task-rest/cleaned_img/cleaned_task-rest_run-{run_id}.nii.gz\")  # 4D fMRI data\n",
    "\n",
    "            if region == 'rsp':\n",
    "                roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/Schaefer/2mm/{region}_all.nii.gz\")\n",
    "            else:\n",
    "                roi_path = os.path.join(roi_path_base, f\"sub-{sub_id}/freesurfer/anat/2mm/{region}_bin.nii.gz\")  # 3D ROI for the given region\n",
    "\n",
    "            image = nib.load(image_path)\n",
    "            roi_image = nib.load(roi_path)\n",
    "\n",
    "            data = image.get_fdata()\n",
    "            roi_data = roi_image.get_fdata()\n",
    "\n",
    "            mask = np.where(roi_data == 1)\n",
    "\n",
    "            voxel_list = []\n",
    "            for t in range(data.shape[-1]):  # For each TR\n",
    "                roi_voxels = data[..., t][mask]  # Extracting voxel values at the current TR\n",
    "                voxel_list.append(roi_voxels)\n",
    "\n",
    "            key = (sub_id, region, run_id)\n",
    "            all_voxels[key] = voxel_list\n",
    "\n",
    "# Output voxel data as separate CSVs\n",
    "for (sub_id, region, run_id), voxels in all_voxels.items():\n",
    "    df = pd.DataFrame(voxels).transpose()  # Transpose dataframe\n",
    "    df.columns = ['TR_%d' % (i+1) for i in range(df.shape[1])]  # Adding column names\n",
    "    df.index = ['voxel_%d' % (i+1) for i in range(df.shape[0])]  # Adding index names\n",
    "\n",
    "    output_dir = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{sub_id}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f'voxBYtr_{region}_sub-{sub_id}_run-{run_id}_day1rest.csv')\n",
    "    df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##corr matrix generation for all subjects and regions \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration for the runs, days, and regions\n",
    "combinations = [\n",
    "    (\"03\", \"day2\", \"02\"),\n",
    "    (\"01\", \"day1\", \"01\"),\n",
    "    (\"01\", \"day1\", \"02\"),\n",
    "    (\"01\", \"day2\", \"01\"),\n",
    "    (\"01\", \"day2\", \"02\"),\n",
    "    (\"02\", \"day2\", \"01\")\n",
    "]\n",
    "\n",
    "all_regions = ['vtc', 'peri', 'rsp']\n",
    "all_participants = [\"%02d\" % x for x in range(2, 31)]  # For all subjects from 02 to 30\n",
    "\n",
    "for participant_str in all_participants:\n",
    "    for region in all_regions:\n",
    "        for tr_run, tr_day, trial_run in combinations:\n",
    "            # Setting filenames for the current combination\n",
    "            trial_input_file = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{participant_str}/voxBYtr_{region}_sub-{participant_str}_run-{tr_run}_{tr_day}rest.csv'\n",
    "            tr_input_file = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{participant_str}/voxBYtrial_{region}_sub-{participant_str}_run-{trial_run}_day2enc.csv'\n",
    "\n",
    "            # Check if the files exist, if not, continue to the next combination\n",
    "            if not (os.path.exists(trial_input_file) and os.path.exists(tr_input_file)):\n",
    "                print(f\"Files for participant {participant_str}, region {region}, TR Run-{tr_run} {tr_day} by Trial Run-{trial_run} not found!\")\n",
    "                continue\n",
    "\n",
    "            # Read the CSV files into DataFrames\n",
    "            df1 = pd.read_csv(trial_input_file)\n",
    "            df2 = pd.read_csv(tr_input_file)\n",
    "\n",
    "            # Drop the first column from df2\n",
    "            df2 = df2.iloc[:, 1:]\n",
    "\n",
    "            # Convert to numeric, ignoring errors\n",
    "            df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "            df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Concatenate the two dataframes along the columns (TRs first, then Trials)\n",
    "            df = pd.concat([df2, df1], axis=1)\n",
    "\n",
    "            # Compute the correlation matrix\n",
    "            correlation_matrix = df.corr()\n",
    "\n",
    "            # Extract the portion of the correlation matrix corresponding to TR and Trial\n",
    "            correlation_trial_tr = correlation_matrix.iloc[-210:, :df2.shape[1]]\n",
    "\n",
    "            # Set row names to 'TR_01' through 'TR_210'\n",
    "            correlation_trial_tr.index = [f'TR_{i:02}' for i in range(1, 211)]\n",
    "            # Set column names to 'Trial_01' through the appropriate TR based on the extracted matrix size\n",
    "            correlation_trial_tr.columns = [f'Trial_{i:02}' for i in range(1, df2.shape[1] + 1)]\n",
    "\n",
    "            # Save correlation matrix as a CSV file with row names and column names\n",
    "            output_folder = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{participant_str}/{region}_corr'\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "            output_filename = os.path.join(output_folder, f'corr_matrix_{region}_TR_Run-{tr_run}_{tr_day}_by_Trial-Run-{trial_run}.csv')\n",
    "            correlation_trial_tr.to_csv(output_filename, index=True, header=True)\n",
    "\n",
    "            print(f\"Correlation matrix saved for participant {participant_str}, region {region}, TR Run-{tr_run} {tr_day} by Trial Run-{trial_run}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this is last this is encoding Trial by encoding Trial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of all regions\n",
    "all_regions = ['vtc', 'peri', 'rsp']\n",
    "\n",
    "# List of all participants from 02 to 30\n",
    "all_participants = [\"%02d\" % x for x in range(2, 31)]\n",
    "\n",
    "# Loop through participants\n",
    "for participant_str in all_participants:\n",
    "\n",
    "    # Loop through runs '01' and '02'\n",
    "    for run in ['01', '02']:\n",
    "        \n",
    "        # Loop through the specified regions\n",
    "        for region in all_regions:\n",
    "\n",
    "            # Define the file path for the current region\n",
    "            input_path = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{participant_str}/voxBYtrial_{region}_sub-{participant_str}_run-{run}_day2enc.csv'\n",
    "            \n",
    "            df1 = pd.read_csv(input_path)\n",
    "\n",
    "            # Drop any unnecessary columns, such as 'Unnamed: 0'\n",
    "            if 'Unnamed: 0' in df1.columns:\n",
    "                df1 = df1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "            # Ensure that df1 contains exactly 54 columns\n",
    "            if df1.shape[1] != 54:\n",
    "                raise ValueError(f\"Expected 54 columns for {input_path}, but found {df1.shape[1]} columns\")\n",
    "\n",
    "            # Convert to numeric, ignoring errors\n",
    "            df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Compute the correlation matrix for the trial by trial section\n",
    "            correlation_matrix_trial_by_trial = df1.corr()\n",
    "\n",
    "            # Set column and row names according to your desired pattern\n",
    "            correlation_matrix_trial_by_trial.columns = [f'Trial_{i:02}' for i in range(1, 55)]\n",
    "            correlation_matrix_trial_by_trial.index = [f'Trial_{i:02}' for i in range(1, 55)]\n",
    "\n",
    "            # Define output folder and file name\n",
    "            output_folder = f'/data/Kathryn/Projects/Priority/derivatives/rsa_dfs/{region}/sub-{participant_str}/{region}_corr'\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            output_filename = os.path.join(output_folder, f'corr_matrix_{region}_Trial_by_Trial_Run-{run}.csv')\n",
    "\n",
    "            # Save correlation matrix as a CSV file with row names and column names\n",
    "            correlation_matrix_trial_by_trial.to_csv(output_filename, index=True, header=True)\n",
    "\n",
    "            print(f\"Correlation matrix saved for Subject {participant_str}, Run {run}, Region {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##renaming cols for encoding trial 1 \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base path\n",
    "base_dir = \"/data/Kathryn/Projects/Priority/derivatives/rsa_dfs\"\n",
    "\n",
    "# Regions and subjects\n",
    "all_regions = ['vtc', 'peri', 'rsp']\n",
    "all_subjects = [f\"sub-{i:02d}\" for i in range(2, 31)]  # sub-02 to sub-30\n",
    "\n",
    "for region in all_regions:\n",
    "    for subject in all_subjects:\n",
    "        \n",
    "        # Constructed base path for each subject and region\n",
    "        base_path = os.path.join(base_dir, region, subject, f\"{region}_corr\")\n",
    "        \n",
    "        # File names for source (from hipp for the current subject) and targets\n",
    "        source_file = os.path.join(base_dir, 'hipp', subject, 'hipp_corr', f'corr_matrix_hipp_TR_Run-02_day2_by_Trial-Run-01.csv')\n",
    "        target_files = [\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-01_day1_by_Trial-Run-01.csv'),\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-01_day2_by_Trial-Run-01.csv'),\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-02_day2_by_Trial-Run-01.csv')\n",
    "        ]\n",
    "\n",
    "        # Check if source file exists before processing\n",
    "        if os.path.exists(source_file):\n",
    "            # Read source file and get the column names\n",
    "            source_df = pd.read_csv(source_file)\n",
    "            cols = source_df.columns\n",
    "\n",
    "            # Apply column names to target files\n",
    "            for file in target_files:\n",
    "                if os.path.exists(file):  # Check if the target file exists\n",
    "                    df = pd.read_csv(file)  # Read with existing headers\n",
    "                    df.columns = cols\n",
    "                    df.to_csv(file, index=False)\n",
    "            print(f\"Column names replaced for {subject} in {region}!\")\n",
    "        else:\n",
    "            print(f\"Source file for {subject} in {region} not found!\")\n",
    "\n",
    "print(\"Process completed!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##renaming cols for encoding trial 2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base path\n",
    "base_dir = \"/data/Kathryn/Projects/Priority/derivatives/rsa_dfs\"\n",
    "\n",
    "# Regions and subjects\n",
    "all_regions = ['vtc', 'peri', 'rsp']\n",
    "all_subjects = [f\"sub-{i:02d}\" for i in range(2, 31)]  # sub-02 to sub-30\n",
    "\n",
    "for region in all_regions:\n",
    "    for subject in all_subjects:\n",
    "        \n",
    "        # Constructed base path for each subject and region\n",
    "        base_path = os.path.join(base_dir, region, subject, f\"{region}_corr\")\n",
    "        \n",
    "        # File names for source (from hipp for the current subject) and targets\n",
    "        source_file = os.path.join(base_dir, 'hipp', subject, 'hipp_corr', f'corr_matrix_hipp_TR_Run-03_day2_by_Trial-Run-02.csv')\n",
    "        target_files = [\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-01_day1_by_Trial-Run-02.csv'),\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-01_day2_by_Trial-Run-02.csv'),\n",
    "            os.path.join(base_path, f'corr_matrix_{region}_TR_Run-03_day2_by_Trial-Run-02.csv')\n",
    "        ]\n",
    "\n",
    "        # Check if source file exists before processing\n",
    "        if os.path.exists(source_file):\n",
    "            # Read source file and get the column names\n",
    "            source_df = pd.read_csv(source_file)\n",
    "            cols = source_df.columns\n",
    "\n",
    "            # Apply column names to target files\n",
    "            for file in target_files:\n",
    "                if os.path.exists(file):  # Check if the target file exists\n",
    "                    df = pd.read_csv(file)  # Read with existing headers\n",
    "                    df.columns = cols\n",
    "                    df.to_csv(file, index=False)\n",
    "            print(f\"Column names replaced for {subject} in {region}!\")\n",
    "        else:\n",
    "            print(f\"Source file for {subject} in {region} not found!\")\n",
    "\n",
    "print(\"Process completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm_pri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
